{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWMz3Dwoz0fA"
   },
   "source": [
    "Some helpful imports. Installing `geopandas` can be a bit of a challenge sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import cvxpy as cp\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import re\n",
    "import os, sys\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import gurobipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell reads in the Cook County shapefile and somes some relabelling to get it into something which is marginally human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = gpd.read_file(\"cook_il/shapefiles/cook_il_race_hisp.shp\")\n",
    "\n",
    "popcol = \"total\"\n",
    "geolevels = [\"state\",\"county\",\"subcounty\",\"tractgroup\",\"tract\",\"blockgroup\",\"block\"]\n",
    "\n",
    "\n",
    "cols = [\n",
    " 'Id2',\n",
    " 'Total_',\n",
    " 'Population',\n",
    " 'Populati_1',\n",
    " 'Populati_2',\n",
    " 'Populati_3',\n",
    " 'Populati_4',\n",
    " 'Populati_5',\n",
    " 'Populati_6',\n",
    " 'Two or Mor',\n",
    " 'Two or M_1',\n",
    " 'Two or M_2',\n",
    " 'Two or M_3',\n",
    " 'Two or M_4',\n",
    " 'Two or M_5',\n",
    " 'Two or M_6',\n",
    " 'Two or M_7',\n",
    " 'Two or M_8',\n",
    " 'Two or M_9',\n",
    " 'Two or M10',\n",
    " 'Two or M11',\n",
    " 'Two or M12',\n",
    " 'Two or M13',\n",
    " 'Two or M14',\n",
    " 'Two or M15',\n",
    " 'Two or M16',\n",
    " 'Two or M17',\n",
    " 'Two or M18',\n",
    " 'Two or M19',\n",
    " 'Two or M20',\n",
    " 'Two or M21',\n",
    " 'Two or M22',\n",
    " 'Two or M23',\n",
    " 'Two or M24',\n",
    " 'Two or M25',\n",
    " 'Two or M26',\n",
    " 'Two or M27',\n",
    " 'Two or M28',\n",
    " 'Two or M29',\n",
    " 'Two or M30',\n",
    " 'Two or M31',\n",
    " 'Two or M32',\n",
    " 'Two or M33',\n",
    " 'Two or M34',\n",
    " 'Two or M35',\n",
    " 'Two or M36',\n",
    " 'Two or M37',\n",
    " 'Two or M38',\n",
    " 'Two or M39',\n",
    " 'Two or M40',\n",
    " 'Two or M41',\n",
    " 'Two or M42',\n",
    " 'Two or M43',\n",
    " 'Two or M44',\n",
    " 'Two or M45',\n",
    " 'Two or M46',\n",
    " 'Two or M47',\n",
    " 'Two or M48',\n",
    " 'Two or M49',\n",
    " 'Two or M50',\n",
    " 'Two or M51',\n",
    " 'Two or M52',\n",
    " 'Two or M53',\n",
    " 'Two or M54',\n",
    " 'Two or M55',\n",
    " 'Two or M56',\n",
    " 'Two or M57',\n",
    " 'Two or M58',\n",
    " 'Two or M59',\n",
    " 'Two or M60',\n",
    " 'Two or M61',\n",
    " 'Two or M62',\n",
    " 'Hispanic o',\n",
    " 'Not Hispan',\n",
    " 'Not Hisp_1',\n",
    " 'Not Hisp_2',\n",
    " 'Not Hisp_3',\n",
    " 'Not Hisp_4',\n",
    " 'Not Hisp_5',\n",
    " 'Not Hisp_6',\n",
    " 'Not Hisp_7',\n",
    " 'Not Hisp_8',\n",
    " 'Not Hisp_9',\n",
    " 'Not Hisp10',\n",
    " 'Not Hisp11',\n",
    " 'Not Hisp12',\n",
    " 'Not Hisp13',\n",
    " 'Not Hisp14',\n",
    " 'Not Hisp15',\n",
    " 'Not Hisp16',\n",
    " 'Not Hisp17',\n",
    " 'Not Hisp18',\n",
    " 'Not Hisp19',\n",
    " 'Not Hisp20',\n",
    " 'Not Hisp21',\n",
    " 'Not Hisp22',\n",
    " 'Not Hisp23',\n",
    " 'Not Hisp24',\n",
    " 'Not Hisp25',\n",
    " 'Not Hisp26',\n",
    " 'Not Hisp27',\n",
    " 'Not Hisp28',\n",
    " 'Not Hisp29',\n",
    " 'Not Hisp30',\n",
    " 'Not Hisp31',\n",
    " 'Not Hisp32',\n",
    " 'Not Hisp33',\n",
    " 'Not Hisp34',\n",
    " 'Not Hisp35',\n",
    " 'Not Hisp36',\n",
    " 'Not Hisp37',\n",
    " 'Not Hisp38',\n",
    " 'Not Hisp39',\n",
    " 'Not Hisp40',\n",
    " 'Not Hisp41',\n",
    " 'Not Hisp42',\n",
    " 'Not Hisp43',\n",
    " 'Not Hisp44',\n",
    " 'Not Hisp45',\n",
    " 'Not Hisp46',\n",
    " 'Not Hisp47',\n",
    " 'Not Hisp48',\n",
    " 'Not Hisp49',\n",
    " 'Not Hisp50',\n",
    " 'Not Hisp51',\n",
    " 'Not Hisp52',\n",
    " 'Not Hisp53',\n",
    " 'Not Hisp54',\n",
    " 'Not Hisp55',\n",
    " 'Not Hisp56',\n",
    " 'Not Hisp57',\n",
    " 'Not Hisp58',\n",
    " 'Not Hisp59',\n",
    " 'Not Hisp60',\n",
    " 'Not Hisp61',\n",
    " 'Not Hisp62',\n",
    " 'Not Hisp63',\n",
    " 'Not Hisp64',\n",
    " 'Not Hisp65',\n",
    " 'Not Hisp66',\n",
    " 'Not Hisp67',\n",
    " 'Not Hisp68',\n",
    " 'Not Hisp69',\n",
    " 'Not Hisp70',\n",
    " 'geometry']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_col_names = [\n",
    "'id',\n",
    "'total',\n",
    "'r_1',\n",
    "'r_1-w',\n",
    "'r_1-b',\n",
    "'r_1-amin',\n",
    "'r_1-a',\n",
    "'r_1-nhpi',\n",
    "'r_1-other',\n",
    "'r_2_plus',\n",
    "'r_2',\n",
    "'r_2-w-b',\n",
    "'r_2-w-amin',\n",
    "'r_2-w-a',\n",
    "'r_2-w-nhpi',\n",
    "'r_2-w-other',\n",
    "'r_2-b-amin',\n",
    "'r_2-b-a',\n",
    "'r_2-b-nhpi',\n",
    "'r_2-b-other',\n",
    "'r_2-amin-a',\n",
    "'r_2-amin-nhpi',\n",
    "'r_2-amin-other',\n",
    "'r_2-a-nhpi',\n",
    "'r_2-a-other',\n",
    "'r_2-nhpi-other',\n",
    "'r_3',\n",
    "'r_3-w-b-amin',\n",
    "'r_3-w-b-a',\n",
    "'r_3-w-b-nhpi',\n",
    "'r_3-w-b-other',\n",
    "'r_3-w-amin-a',\n",
    "'r_3-w-amin-nhpi',\n",
    "'r_3-w-amin-other',\n",
    "'r_3-w-a-nhpi',\n",
    "'r_3-w-a-other',\n",
    "'r_3-w-nhpi-other',\n",
    "'r_3-b-amin-a',\n",
    "'r_3-b-amin-nhpi',\n",
    "'r_3-b-amin-other',\n",
    "'r_3-b-a-nhpi',\n",
    "'r_3-b-a-other',\n",
    "'r_3-b-nhpi-other',\n",
    "'r_3-amin-a-nhpi',\n",
    "'r_3-amin-a-other',\n",
    "'r_3-amin-nhpi-other',\n",
    "'r_3-a-nhpi-other',\n",
    "'r_4',\n",
    "'r_4-w-b-amin-a',\n",
    "'r_4-w-b-amin-nhpi',\n",
    "'r_4-w-b-amin-other',\n",
    "'r_4-w-b-a-nhpi',\n",
    "'r_4-w-b-a-other',\n",
    "'r_4-w-b-nhpi-other',\n",
    "'r_4-w-amin-a-nhpi',\n",
    "'r_4-w-amin-a-other',\n",
    "'r_4-w-amin-nhpi-other',\n",
    "'r_4-w-a-nhpi-other',\n",
    "'r_4-b-amin-a-nhpi',\n",
    "'r_4-b-amin-a-other',\n",
    "'r_4-b-amin-nhpi-other',\n",
    "'r_4-b-a-nhpi-other',\n",
    "'r_4-amin-a-nhpi-other',\n",
    "'r_5',\n",
    "'r_5-w-b-amin-a-nhpi',\n",
    "'r_5-w-b-amin-a-other',\n",
    "'r_5-w-b-amin-nhpi-other',\n",
    "'r_5-w-b-a-nhpi-other',\n",
    "'r_5-w-amin-a-nhpi-other',\n",
    "'r_5-b-amin-a-nhpi-other',\n",
    "'r_6',\n",
    "'r_6-w-b-amin-a-nhpi-other',\n",
    "'hisp',\n",
    "'nohisp',\n",
    "'nohisp_r_1',\n",
    "'nohisp_r_1-w',\n",
    "'nohisp_r_1-b',\n",
    "'nohisp_r_1-amin',\n",
    "'nohisp_r_1-a',\n",
    "'nohisp_r_1-nhpi',\n",
    "'nohisp_r_1-other',\n",
    "'nohisp_r_2_plus',\n",
    "'nohisp_r_2',\n",
    "'nohisp_r_2-w-b',\n",
    "'nohisp_r_2-w-amin',\n",
    "'nohisp_r_2-w-a',\n",
    "'nohisp_r_2-w-nhpi',\n",
    "'nohisp_r_2-w-other',\n",
    "'nohisp_r_2-b-amin',\n",
    "'nohisp_r_2-b-a',\n",
    "'nohisp_r_2-b-nhpi',\n",
    "'nohisp_r_2-b-other',\n",
    "'nohisp_r_2-amin-a',\n",
    "'nohisp_r_2-amin-nhpi',\n",
    "'nohisp_r_2-amin-other',\n",
    "'nohisp_r_2-a-nhpi',\n",
    "'nohisp_r_2-a-other',\n",
    "'nohisp_r_2-nhpi-other',\n",
    "'nohisp_r_3',\n",
    "'nohisp_r_3-w-b-amin',\n",
    "'nohisp_r_3-w-b-a',\n",
    "'nohisp_r_3-w-b-nhpi',\n",
    "'nohisp_r_3-w-b-other',\n",
    "'nohisp_r_3-w-amin-a',\n",
    "'nohisp_r_3-w-amin-nhpi',\n",
    "'nohisp_r_3-w-amin-other',\n",
    "'nohisp_r_3-w-a-nhpi',\n",
    "'nohisp_r_3-w-a-other',\n",
    "'nohisp_r_3-w-nhpi-other',\n",
    "'nohisp_r_3-b-amin-a',\n",
    "'nohisp_r_3-b-amin-nhpi',\n",
    "'nohisp_r_3-b-amin-other',\n",
    "'nohisp_r_3-b-a-nhpi',\n",
    "'nohisp_r_3-b-a-other',\n",
    "'nohisp_r_3-b-nhpi-other',\n",
    "'nohisp_r_3-amin-a-nhpi',\n",
    "'nohisp_r_3-amin-a-other',\n",
    "'nohisp_r_3-amin-nhpi-other',\n",
    "'nohisp_r_3-a-nhpi-other',\n",
    "'nohisp_r_4',\n",
    "'nohisp_r_4-w-b-amin-a',\n",
    "'nohisp_r_4-w-b-amin-nhpi',\n",
    "'nohisp_r_4-w-b-amin-other',\n",
    "'nohisp_r_4-w-b-a-nhpi',\n",
    "'nohisp_r_4-w-b-a-other',\n",
    "'nohisp_r_4-w-b-nhpi-other',\n",
    "'nohisp_r_4-w-amin-a-nhpi',\n",
    "'nohisp_r_4-w-amin-a-other',\n",
    "'nohisp_r_4-w-amin-nhpi-other',\n",
    "'nohisp_r_4-w-a-nhpi,-other',\n",
    "'nohisp_r_4-b-amin-a-nhpi',\n",
    "'nohisp_r_4-b-amin-a-other',\n",
    "'nohisp_r_4-b-amin-nhpi-other',\n",
    "'nohisp_r_4-b-a-nhpi-other',\n",
    "'nohisp_r_4-amin-a-nhpi-other',\n",
    "'nohisp_r_5',\n",
    "'nohisp_r_5-w-b-amin-a-nhpi',\n",
    "'nohisp_r_5-w-b-amin-a-other',\n",
    "'nohisp_r_5-w-b-amin-nhpi-other',\n",
    "'nohisp_r_5-w-b-a-nhpi-other',\n",
    "'nohisp_r_5-w-amin-a-nhpi-other',\n",
    "'nohisp_r_5-b-amin-a-nhpi-other',\n",
    "'nohisp_r_6',\n",
    "'nohisp_r_6-w-b-amin-a-nhpi-other',\n",
    "'geometry'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_threeplus = [\n",
    "    'nohisp_r_3',\n",
    "'nohisp_r_3-w-b-amin',\n",
    "'nohisp_r_3-w-b-a',\n",
    "'nohisp_r_3-w-b-nhpi',\n",
    "'nohisp_r_3-w-b-other',\n",
    "'nohisp_r_3-w-amin-a',\n",
    "'nohisp_r_3-w-amin-nhpi',\n",
    "'nohisp_r_3-w-amin-other',\n",
    "'nohisp_r_3-w-a-nhpi',\n",
    "'nohisp_r_3-w-a-other',\n",
    "'nohisp_r_3-w-nhpi-other',\n",
    "'nohisp_r_3-b-amin-a',\n",
    "'nohisp_r_3-b-amin-nhpi',\n",
    "'nohisp_r_3-b-amin-other',\n",
    "'nohisp_r_3-b-a-nhpi',\n",
    "'nohisp_r_3-b-a-other',\n",
    "'nohisp_r_3-b-nhpi-other',\n",
    "'nohisp_r_3-amin-a-nhpi',\n",
    "'nohisp_r_3-amin-a-other',\n",
    "'nohisp_r_3-amin-nhpi-other',\n",
    "'nohisp_r_3-a-nhpi-other',\n",
    "'nohisp_r_4',\n",
    "'nohisp_r_4-w-b-amin-a',\n",
    "'nohisp_r_4-w-b-amin-nhpi',\n",
    "'nohisp_r_4-w-b-amin-other',\n",
    "'nohisp_r_4-w-b-a-nhpi',\n",
    "'nohisp_r_4-w-b-a-other',\n",
    "'nohisp_r_4-w-b-nhpi-other',\n",
    "'nohisp_r_4-w-amin-a-nhpi',\n",
    "'nohisp_r_4-w-amin-a-other',\n",
    "'nohisp_r_4-w-amin-nhpi-other',\n",
    "'nohisp_r_4-w-a-nhpi,-other',\n",
    "'nohisp_r_4-b-amin-a-nhpi',\n",
    "'nohisp_r_4-b-amin-a-other',\n",
    "'nohisp_r_4-b-amin-nhpi-other',\n",
    "'nohisp_r_4-b-a-nhpi-other',\n",
    "'nohisp_r_4-amin-a-nhpi-other',\n",
    "'nohisp_r_5',\n",
    "'nohisp_r_5-w-b-amin-a-nhpi',\n",
    "'nohisp_r_5-w-b-amin-a-other',\n",
    "'nohisp_r_5-w-b-amin-nhpi-other',\n",
    "'nohisp_r_5-w-b-a-nhpi-other',\n",
    "'nohisp_r_5-w-amin-a-nhpi-other',\n",
    "'nohisp_r_5-b-amin-a-nhpi-other',\n",
    "'nohisp_r_6',\n",
    "'nohisp_r_6-w-b-amin-a-nhpi-other',\n",
    "    \n",
    "'r_3',\n",
    "'r_3-w-b-amin',\n",
    "'r_3-w-b-a',\n",
    "'r_3-w-b-nhpi',\n",
    "'r_3-w-b-other',\n",
    "'r_3-w-amin-a',\n",
    "'r_3-w-amin-nhpi',\n",
    "'r_3-w-amin-other',\n",
    "'r_3-w-a-nhpi',\n",
    "'r_3-w-a-other',\n",
    "'r_3-w-nhpi-other',\n",
    "'r_3-b-amin-a',\n",
    "'r_3-b-amin-nhpi',\n",
    "'r_3-b-amin-other',\n",
    "'r_3-b-a-nhpi',\n",
    "'r_3-b-a-other',\n",
    "'r_3-b-nhpi-other',\n",
    "'r_3-amin-a-nhpi',\n",
    "'r_3-amin-a-other',\n",
    "'r_3-amin-nhpi-other',\n",
    "'r_3-a-nhpi-other',\n",
    "'r_4',\n",
    "'r_4-w-b-amin-a',\n",
    "'r_4-w-b-amin-nhpi',\n",
    "'r_4-w-b-amin-other',\n",
    "'r_4-w-b-a-nhpi',\n",
    "'r_4-w-b-a-other',\n",
    "'r_4-w-b-nhpi-other',\n",
    "'r_4-w-amin-a-nhpi',\n",
    "'r_4-w-amin-a-other',\n",
    "'r_4-w-amin-nhpi-other',\n",
    "'r_4-w-a-nhpi-other',\n",
    "'r_4-b-amin-a-nhpi',\n",
    "'r_4-b-amin-a-other',\n",
    "'r_4-b-amin-nhpi-other',\n",
    "'r_4-b-a-nhpi-other',\n",
    "'r_4-amin-a-nhpi-other',\n",
    "'r_5',\n",
    "'r_5-w-b-amin-a-nhpi',\n",
    "'r_5-w-b-amin-a-other',\n",
    "'r_5-w-b-amin-nhpi-other',\n",
    "'r_5-w-b-a-nhpi-other',\n",
    "'r_5-w-amin-a-nhpi-other',\n",
    "'r_5-b-amin-a-nhpi-other',\n",
    "'r_6',\n",
    "'r_6-w-b-amin-a-nhpi-other',    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "noisy_cols = [\n",
    "'total',\n",
    "'r_1',\n",
    "'r_1-w',\n",
    "'r_1-b',\n",
    "'r_1-amin',\n",
    "'r_1-a',\n",
    "'r_1-nhpi',\n",
    "'r_1-other',\n",
    "'r_2_plus',\n",
    "'r_2',\n",
    "'r_2-w-b',\n",
    "'r_2-w-amin',\n",
    "'r_2-w-a',\n",
    "'r_2-w-nhpi',\n",
    "'r_2-w-other',\n",
    "'r_2-b-amin',\n",
    "'r_2-b-a',\n",
    "'r_2-b-nhpi',\n",
    "'r_2-b-other',\n",
    "'r_2-amin-a',\n",
    "'r_2-amin-nhpi',\n",
    "'r_2-amin-other',\n",
    "'r_2-a-nhpi',\n",
    "'r_2-a-other',\n",
    "'r_2-nhpi-other',\n",
    "'r_3_plus',\n",
    "'hisp',\n",
    "'nohisp',\n",
    "'nohisp_r_1',\n",
    "'nohisp_r_1-w',\n",
    "'nohisp_r_1-b',\n",
    "'nohisp_r_1-amin',\n",
    "'nohisp_r_1-a',\n",
    "'nohisp_r_1-nhpi',\n",
    "'nohisp_r_1-other',\n",
    "'nohisp_r_2_plus',\n",
    "'nohisp_r_2',\n",
    "'nohisp_r_2-w-b',\n",
    "'nohisp_r_2-w-amin',\n",
    "'nohisp_r_2-w-a',\n",
    "'nohisp_r_2-w-nhpi',\n",
    "'nohisp_r_2-w-other',\n",
    "'nohisp_r_2-b-amin',\n",
    "'nohisp_r_2-b-a',\n",
    "'nohisp_r_2-b-nhpi',\n",
    "'nohisp_r_2-b-other',\n",
    "'nohisp_r_2-amin-a',\n",
    "'nohisp_r_2-amin-nhpi',\n",
    "'nohisp_r_2-amin-other',\n",
    "'nohisp_r_2-a-nhpi',\n",
    "'nohisp_r_2-a-other',\n",
    "'nohisp_r_2-nhpi-other',\n",
    "'nohisp_r_3_plus'\n",
    "]\n",
    "\n",
    "\n",
    "check_cols = [\n",
    "'r_1',\n",
    "'r_1-w',\n",
    "'r_1-b',\n",
    "'r_1-amin',\n",
    "'r_1-a',\n",
    "'r_1-nhpi',\n",
    "'r_1-other',\n",
    "'r_2_plus',\n",
    "'r_2',\n",
    "'r_2-w-b',\n",
    "'r_2-w-amin',\n",
    "'r_2-w-a',\n",
    "'r_2-w-nhpi',\n",
    "'r_2-w-other',\n",
    "'r_2-b-amin',\n",
    "'r_2-b-a',\n",
    "'r_2-b-nhpi',\n",
    "'r_2-b-other',\n",
    "'r_2-amin-a',\n",
    "'r_2-amin-nhpi',\n",
    "'r_2-amin-other',\n",
    "'r_2-a-nhpi',\n",
    "'r_2-a-other',\n",
    "'r_2-nhpi-other',\n",
    "'r_3_plus',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vert_cols = [\n",
    "    \n",
    "    \n",
    "'r_1-w',\n",
    "'r_1-b',\n",
    "'r_1-amin',\n",
    "'r_1-a',\n",
    "'r_1-nhpi',\n",
    "'r_1-other',\n",
    "\n",
    "'r_2-w-b',\n",
    "'r_2-w-amin',\n",
    "'r_2-w-a',\n",
    "'r_2-w-nhpi',\n",
    "'r_2-w-other',\n",
    "'r_2-b-amin',\n",
    "'r_2-b-a',\n",
    "'r_2-b-nhpi',\n",
    "'r_2-b-other',\n",
    "'r_2-amin-a',\n",
    "'r_2-amin-nhpi',\n",
    "'r_2-amin-other',\n",
    "'r_2-a-nhpi',\n",
    "'r_2-a-other',\n",
    "'r_2-nhpi-other',\n",
    "'r_3_plus',\n",
    "\n",
    "'nohisp_r_1-w',\n",
    "'nohisp_r_1-b',\n",
    "'nohisp_r_1-amin',\n",
    "'nohisp_r_1-a',\n",
    "'nohisp_r_1-nhpi',\n",
    "'nohisp_r_1-other',\n",
    "'nohisp_r_2-w-b',\n",
    "'nohisp_r_2-w-amin',\n",
    "'nohisp_r_2-w-a',\n",
    "'nohisp_r_2-w-nhpi',\n",
    "'nohisp_r_2-w-other',\n",
    "'nohisp_r_2-b-amin',\n",
    "'nohisp_r_2-b-a',\n",
    "'nohisp_r_2-b-nhpi',\n",
    "'nohisp_r_2-b-other',\n",
    "'nohisp_r_2-amin-a',\n",
    "'nohisp_r_2-amin-nhpi',\n",
    "'nohisp_r_2-amin-other',\n",
    "'nohisp_r_2-a-nhpi',\n",
    "'nohisp_r_2-a-other',\n",
    "'nohisp_r_2-nhpi-other',\n",
    "'nohisp_r_3_plus'\n",
    "    \n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df = df.rename(columns={  cols[i] : new_col_names[i] for i in range(len(cols))    })\n",
    "\n",
    "\n",
    "df = df[new_col_names]\n",
    "\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "for c in new_col_names[1:-1]:\n",
    "    df[\"new\"] = df[c].str.replace('\\(([^)]+)\\)',\"\",regex=True)\n",
    "    \n",
    "    df[c] = df[\"new\"].astype(\"float\")\n",
    "\n",
    "\n",
    "\n",
    "## collapse all r_3++ into new r_3_plus\n",
    "df['r_3_plus'] = (df['r_3'] + df['r_4'] + df['r_5'] + df[\"r_6\"]).astype(int)\n",
    "\n",
    "df['nohisp_r_3_plus'] = (df['nohisp_r_3'] + df['nohisp_r_4'] + df['nohisp_r_5'] + df['nohisp_r_6'])\n",
    "\n",
    "\n",
    "## drop extra r_3,4,5,6 cols\n",
    "df = df.drop(columns = cols_threeplus)\n",
    "\n",
    "df = df[df[\"total\"] != 0]\n",
    "\n",
    "\n",
    "\n",
    "# add geographic hierarchy columns:\n",
    "\n",
    "\n",
    "df[\"state\"]      = df[\"id\"].str[:2]\n",
    "df[\"county\"]     = df[\"id\"].str[:5]\n",
    "df[\"subcounty\"]  = df[\"id\"].str[:7]\n",
    "df[\"tractgroup\"] = df[\"id\"].str[:9]\n",
    "df[\"tract\"]      = df[\"id\"].str[:11]\n",
    "df[\"blockgroup\"] = df[\"id\"].str[:12]\n",
    "df[\"block\"]      = df[\"id\"].str[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell does some quick sanity checks of the data, making sure that the constraints we are about to define are satisfied by the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data checks: should all of these should be 1\n",
    "\n",
    "\n",
    "## check that everyone is 1, or 2+ races\n",
    "print(np.prod(df[\"total\"] == df[\"r_1\"] + df[\"r_2_plus\"]))\n",
    "print(np.prod(df[\"nohisp\"] == df[\"nohisp_r_1\"] + df[\"nohisp_r_2_plus\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## check that everyone is 1, 2, or 3+ races\n",
    "print(np.prod(df[\"total\"] == df[\"r_1\"] + df[\"r_2\"] + df[\"r_3_plus\"]))\n",
    "print(np.prod(df[\"nohisp\"] == df[\"nohisp_r_1\"] + df[\"nohisp_r_2\"] + df[\"nohisp_r_3_plus\"]))\n",
    "\n",
    "\n",
    "## check that \"2+\" equals 2 and 3+\n",
    "print(np.prod(df[\"r_2_plus\"] ==  df[\"r_2\"] + df[\"r_3_plus\"]))\n",
    "print(np.prod(df[\"nohisp_r_2_plus\"] ==  df[\"nohisp_r_2\"] + df[\"nohisp_r_3_plus\"]))\n",
    "\n",
    "## check that everyone is either hisp/lat or not\n",
    "print(np.prod(df[\"total\"] == df[\"nohisp\"] + df[\"hisp\"]))\n",
    "\n",
    "\n",
    "## check that all the single-race people are accounted for\n",
    "print(np.prod( df[\"r_1\"] == df['r_1-w'] + df['r_1-b'] + df['r_1-amin'] + df['r_1-a'] + df['r_1-nhpi'] + df['r_1-other']  ))\n",
    "print(np.prod( df[\"nohisp_r_1\"] == df['nohisp_r_1-w'] + df['nohisp_r_1-b'] + df['nohisp_r_1-amin'] + df['nohisp_r_1-a'] + df['nohisp_r_1-nhpi'] + df['nohisp_r_1-other']  ))\n",
    "\n",
    "\n",
    "## check that all the two race people are accounted for\n",
    "print(np.prod(df['r_2'] == df['r_2-w-b'] + df['r_2-w-amin'] + df['r_2-w-a'] + df['r_2-w-nhpi'] + df['r_2-w-other'] + df['r_2-b-amin'] + df['r_2-b-a'] + df['r_2-b-nhpi'] + df['r_2-b-other'] + df['r_2-amin-a'] + df['r_2-amin-nhpi'] + df['r_2-amin-other'] + df['r_2-a-nhpi'] + df['r_2-a-other'] + df['r_2-nhpi-other']))\n",
    "print(np.prod(df['nohisp_r_2'] == df['nohisp_r_2-w-b'] + df['nohisp_r_2-w-amin'] + df['nohisp_r_2-w-a'] + df['nohisp_r_2-w-nhpi'] + df['nohisp_r_2-w-other'] + df['nohisp_r_2-b-amin'] + df['nohisp_r_2-b-a'] + df['nohisp_r_2-b-nhpi'] + df['nohisp_r_2-b-other'] + df['nohisp_r_2-amin-a'] + df['nohisp_r_2-amin-nhpi'] + df['nohisp_r_2-amin-other'] + df['nohisp_r_2-a-nhpi'] + df['nohisp_r_2-a-other'] + df['nohisp_r_2-nhpi-other']))\n",
    "\n",
    "\n",
    "\n",
    "# check that the nohisp values are less than total\n",
    "\n",
    "for c in check_cols:\n",
    "    print(np.prod(df[c]>= df[\"nohisp_\"+c]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hZS_z3YCMDX"
   },
   "source": [
    "Here we define the relevant constraints for within a unit.  Things like \"hispanic population plus non-hispanic population is equal to the total population\" go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPPsXhr3z0fN"
   },
   "outputs": [],
   "source": [
    "# the vertical constraints (sum over all subunits equals unit) will be handled automatically\n",
    "# these are the horizontal constraints \n",
    "# all of these will be linear\n",
    "# form is ( [list of attrs] , relation, [list of attrs]   )\n",
    "# i.e. ( ['_one','_two','_threeplus'], '=' , ['pop']) \n",
    "\n",
    "\n",
    "## invariants\n",
    "\n",
    "invariants = [\n",
    "\n",
    "\n",
    "## check that everyone is 1, or 2+ races\n",
    "([\"total\"], '=', [\"r_1\",\"r_2_plus\"]),\n",
    "([\"nohisp\"] ,'=', [\"nohisp_r_1\",\"nohisp_r_2_plus\"]),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## check that everyone is 1, 2, or 3+ races\n",
    "([\"total\"], '=', [\"r_1\",\"r_2\",\"r_3_plus\"]),\n",
    "([\"nohisp\"] ,'=', [\"nohisp_r_1\",\"nohisp_r_2\",\"nohisp_r_3_plus\"]),\n",
    "\n",
    "\n",
    "\n",
    "## check that \"2+\" equals 2 and 3+\n",
    "([\"r_2_plus\"], '=', [\"r_2\",\"r_3_plus\"]),\n",
    "([\"nohisp_r_2_plus\"], '=', [\"nohisp_r_2\",\"nohisp_r_3_plus\"]),\n",
    "\n",
    "\n",
    "## check that everyone is either hisp/lat or not\n",
    "([\"total\"], '=', [\"nohisp\",\"hisp\"]),\n",
    "\n",
    "\n",
    "## check that all the single-race people are accounted for\n",
    "([\"r_1\"], \"=\",  ['r_1-w','r_1-b','r_1-amin','r_1-a','r_1-nhpi','r_1-other']  ),\n",
    "([\"nohisp_r_1\"] , \"=\", ['nohisp_r_1-w','nohisp_r_1-b','nohisp_r_1-amin','nohisp_r_1-a','nohisp_r_1-nhpi','nohisp_r_1-other']  ),\n",
    "\n",
    "\n",
    "## check that all the two race people are accounted for\n",
    "(['r_2'] ,\"=\", ['r_2-w-b','r_2-w-amin','r_2-w-a','r_2-w-nhpi','r_2-w-other','r_2-b-amin','r_2-b-a','r_2-b-nhpi','r_2-b-other','r_2-amin-a','r_2-amin-nhpi','r_2-amin-other','r_2-a-nhpi','r_2-a-other','r_2-nhpi-other']),\n",
    "(['nohisp_r_2'] ,\"=\", ['nohisp_r_2-w-b','nohisp_r_2-w-amin','nohisp_r_2-w-a','nohisp_r_2-w-nhpi','nohisp_r_2-w-other','nohisp_r_2-b-amin','nohisp_r_2-b-a','nohisp_r_2-b-nhpi','nohisp_r_2-b-other','nohisp_r_2-amin-a','nohisp_r_2-amin-nhpi','nohisp_r_2-amin-other','nohisp_r_2-a-nhpi','nohisp_r_2-a-other','nohisp_r_2-nhpi-other'])\n",
    "\n",
    "\n",
    "\n",
    "# check that the nohisp values are less than total\n",
    "]\n",
    "for c in check_cols:\n",
    "    invariants.append(([\"nohisp_{}\".format(c)], \"<=\", [c]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions to generate random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFAZ1EKWz0fQ"
   },
   "outputs": [],
   "source": [
    "sensitivity = 2\n",
    "\n",
    "\n",
    "def unif_rand(n):\n",
    "    return random.randint(-n,n)\n",
    "\n",
    "def geom_noise(p):\n",
    "    if p == 0: return 0\n",
    "    x =  np.random.geometric(p) - np.random.geometric(p)\n",
    "    return x\n",
    "\n",
    "\n",
    "def add_noise(df, gen, param):\n",
    "    df_noisy = df.copy()\n",
    "    for c in noisy_cols:\n",
    "        noise = [ gen(param) for i in range(len(df))  ]\n",
    "        df_noisy[c] += noise\n",
    "        df_noisy[c] = df_noisy[c].clip(lower=0)\n",
    "    return df_noisy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is going to be useful to precompute the number of blocks constituting each unit.  The `block_counts` dictionary holds that information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XvjVVQ_EG8so",
    "outputId": "642570aa-6cd9-4e70-a287-46436f3c8c5b"
   },
   "outputs": [],
   "source": [
    "block_counts = defaultdict(int)\n",
    "\n",
    "for level in geolevels[:-1]:\n",
    "    print(level, len(set(list(df[level]))))\n",
    "    count = 0\n",
    "    for name in set(list(df[level])):\n",
    "        count +=1\n",
    "        #print(count, end=', ')\n",
    "        block_counts[(level,name)] = df[level].value_counts()[name]\n",
    "    #clear_output()   \n",
    "    \n",
    "def num_blocks(level, name):\n",
    "    #return nc[level].value_counts()[name]\n",
    "    \n",
    "    if level == \"block\": return 1\n",
    "    return block_counts[(level,name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XvjVVQ_EG8so",
    "outputId": "642570aa-6cd9-4e70-a287-46436f3c8c5b"
   },
   "source": [
    "The L_2 optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bcil9h6mz0fS"
   },
   "outputs": [],
   "source": [
    "def solve_l2(df, df_noisy, level, ischild=True, verbose=False):\n",
    "    df_opt = df_noisy.copy()\n",
    "#     print(\"called l2\",level)\n",
    "#     print(df[\"pop\"])\n",
    "#     print(df_noisy[\"pop\"])\n",
    "    \n",
    "    \n",
    "    objective = 0\n",
    "    constraints = []\n",
    "    for c in noisy_cols:\n",
    "        df_opt[c] = list(cp.Variable(len(df_opt[c])))\n",
    "\n",
    "    \n",
    "    for inv in invariants:\n",
    "        #print(inv, end=' ')\n",
    "        if inv[1] == '=':\n",
    "            #print(list(df_opt[inv[0]].sum(axis=1)))\n",
    "           \n",
    "            lhs = list(df_opt[inv[0]].sum(axis=1))\n",
    "            rhs = list(df_opt[inv[2]].sum(axis=1))\n",
    "            for l,r in zip(lhs,rhs):\n",
    "                constraints.append((l-r==0))\n",
    "                \n",
    "        if inv[1] == '<=':\n",
    "            lhs = list(df_opt[inv[0]].sum(axis=1))\n",
    "            rhs = list(df_opt[inv[2]].sum(axis=1))\n",
    "\n",
    "            for l,r in zip(lhs,rhs):\n",
    "                constraints.append((l-r<=0))\n",
    "                \n",
    "    levelids = list(df_noisy[level])\n",
    "    for c in noisy_cols:\n",
    "        #print(c , end = ' ')\n",
    "        if c in vert_cols: constraints.append(  (sum(df_opt[c]) == sum(df[c])    )      )\n",
    "        for n,o,l in zip(list(df_noisy[c]),list(df_opt[c]),levelids):\n",
    "            \n",
    "            \n",
    "            objective += ((n-o)**2)\n",
    "            if c != popcol:\n",
    "                \n",
    "                constraints.append((o>=0))\n",
    "            else:\n",
    "                poptar = num_blocks(level, l     )\n",
    "                constraints.append((o>= float(poptar)))\n",
    "                \n",
    "    #print(\"CONSTRUCTED\")\n",
    "    prob = cp.Problem(cp.Minimize(objective),constraints)\n",
    "    #print(\"INITIALIZED\")\n",
    "    prob.solve( solver=\"GUROBI\", verbose=verbose)\n",
    "    \n",
    "    \n",
    "    for c in noisy_cols:\n",
    "        df_opt[c] = [ np.around(e.value.item(), decimals=5) for e in list(df_opt[c])        ]\n",
    "\n",
    "\n",
    "\n",
    "    return df_opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The L_1 optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJs9TBw7z0fV"
   },
   "outputs": [],
   "source": [
    "def solve_l1(df, df_noisy, level, ischild=True, verbose=False):\n",
    "    \n",
    "\n",
    "    df_opt = df_noisy.copy()\n",
    "#     print(\"called l1\", level)\n",
    "#     print(df[\"pop\"])\n",
    "#     print(df_noisy[\"pop\"])\n",
    "\n",
    "\n",
    "    \n",
    "    objective = 0\n",
    "    constraints = []\n",
    "    \n",
    "    for c in noisy_cols:\n",
    "        df_opt[c] = list(cp.Variable(len(df_opt[c]), integer=True))\n",
    "\n",
    "    \n",
    "    for inv in invariants:\n",
    "        #print(inv, end=' ')\n",
    "\n",
    "        if inv[1] == '=':\n",
    "            #print(list(df_opt[inv[0]].sum(axis=1)))\n",
    "            \n",
    "            lhs = list(df_opt[inv[0]].sum(axis=1))\n",
    "            rhs = list(df_opt[inv[2]].sum(axis=1))\n",
    "\n",
    "            for l,r in zip(lhs,rhs):\n",
    "                constraints.append((l-r==0))\n",
    "                \n",
    "        elif inv[1] == '<=':\n",
    "            lhs = list(df_opt[inv[0]].sum(axis=1))\n",
    "            rhs = list(df_opt[inv[2]].sum(axis=1))\n",
    "\n",
    "            for l,r in zip(lhs,rhs):\n",
    "                constraints.append((l-r<=0))                \n",
    "                \n",
    "                \n",
    "                \n",
    "    levelids = list(df_noisy[level])\n",
    "    for c in noisy_cols:\n",
    "        #print(c , end = ' ')\n",
    "\n",
    "        if c in vert_cols: constraints.append(  (sum(df_opt[c]) == sum(df[c])    )      )\n",
    "        for n,o,l in zip(list(df_noisy[c]),list(df_opt[c]),levelids):\n",
    "            \n",
    "            \n",
    "            objective +=  (( o - np.floor(n) ) * \n",
    "                                 ( n - np.floor(n) ))\n",
    "            constraints.append( (cp.abs( o - n   ) <= 1 )  )\n",
    "\n",
    "            if c != popcol:\n",
    "                constraints.append((o>=0))\n",
    "            else:\n",
    "                poptar = num_blocks(level, l     )\n",
    "                constraints.append((o>= float(poptar)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"CONSTRUCTED\")\n",
    "    prob = cp.Problem(cp.Minimize(objective),constraints)\n",
    "    #print(\"INITIALIZED\")\n",
    "    prob.solve( solver=\"GUROBI\", verbose=verbose)\n",
    "    \n",
    "    \n",
    "    for c in noisy_cols:\n",
    "        df_opt[c] = [ np.rint(e.value.item()) for e in list(df_opt[c])        ]\n",
    "\n",
    "\n",
    "#     print(\"done in {}\".format(datetime.now()-tic))\n",
    "\n",
    "    return df_opt    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orF8QsxHz0fY"
   },
   "source": [
    "A helper function to gather the child dataframes for a given parent unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDZ8OL-Lz0fa"
   },
   "outputs": [],
   "source": [
    "def get_subunit_dfs(df, parent_field, child_field):\n",
    "    d = {}\n",
    "    parents = set(df[parent_field])\n",
    "    \n",
    "    for unit in parents:\n",
    "        df_sub = df.loc[df[parent_field]==unit,:]\n",
    "        d[unit] = (df_sub.groupby(child_field).sum().reset_index() )\n",
    "       \n",
    "    \n",
    "    \n",
    "    return d#{ unit :df.loc[df[parent_field]==unit,:].groupby(child_field).sum().reset_index() for unit in set(df[parent_field]) } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_num_blocks(keys,level):\n",
    "    l = [ block_counts[(k,level)] for k in keys   ]\n",
    "    \n",
    "    return [x for _,x in sorted(zip(l,keys))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop to generate a privatized data frame.  Unless the amount of noise being added is very very large, the optimization problems shouldn't fail to solve.  If the solver fails 50 times in a row on a particular unit (fresh randomness is drawn each time), the whole algorithm will abort.  In generating the data for the paper, we never failed more than twice on any unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXWtHyyfz0fc"
   },
   "outputs": [],
   "source": [
    "failure = None\n",
    "\n",
    "def make_private(df, levels, noise_func, noise_params,verbose):\n",
    "    print(noise_params,levels)\n",
    "    global failure\n",
    "    failure = None\n",
    "    print(\"Starting\")\n",
    "   \n",
    "    _df_tmp = df.groupby(levels[0]).sum().reset_index()\n",
    "    _df_tmp_noisy = add_noise(_df_tmp, noise_func, noise_params[0])\n",
    "    _df_tmp_opt = solve_l2(_df_tmp, _df_tmp_noisy, levels[0], ischild=False, verbose=verbose)\n",
    "    _dfs_tmp_opt = {\"master\": solve_l1(_df_tmp, _df_tmp_opt, levels[0], ischild=False, verbose=verbose)}\n",
    "   \n",
    "    print(\"done {} (top level)\".format(levels[0]))\n",
    "   \n",
    "    for l in range(1,len(levels)):\n",
    "        print(\"starting {}s\".format(levels[l]))\n",
    "        _dfs_dict = get_subunit_dfs(df, levels[l-1], levels[l])\n",
    "        _dfs_dict_noisy = {k:add_noise(v,noise_func, noise_params[l]) for k,v in _dfs_dict.items()}\n",
    "        _dfs_dict_opt = {}\n",
    "        for k in tqdm_notebook(sort_num_blocks(list(_dfs_dict.keys()), levels[l-1])):\n",
    "            #print(\"doing {}s in {}\".format(levels[l], k))\n",
    "       \n",
    "       \n",
    "            for v in _dfs_tmp_opt.values():\n",
    "                df_sub = v.loc[v[levels[l-1]] == k,:]\n",
    "                if df_sub.shape[0] != 0:\n",
    "                    _f = False\n",
    "                    _failcount = 0\n",
    "                    while not _f and _failcount < 50:\n",
    "                        try:\n",
    "                            _t = solve_l2(df_sub, _dfs_dict_noisy[k],levels[l], verbose=verbose)\n",
    "                            _dfs_dict_opt[k] = solve_l1(df_sub,_t, levels[l], verbose=verbose)\n",
    "                            _f = True\n",
    "                        except:\n",
    "                            failure = (df_sub, _dfs_dict_noisy[k],levels[l] )\n",
    "                            _dfs_dict_noisy[k] = add_noise(_dfs_dict[k],noise_func,noise_params[l])\n",
    "                            print(\"FAILED ON {} {} {}\".format(levels[l], k, _failcount))\n",
    "                            _failcount +=1\n",
    "                    if _failcount > 50: return None\n",
    "                   \n",
    "        _dfs_tmp_opt = copy.deepcopy(_dfs_dict_opt)\n",
    "   \n",
    "   \n",
    "    priv_dfs = [v for v in _dfs_tmp_opt.values()]\n",
    "\n",
    "    df_private = priv_dfs[0]\n",
    "    for priv_df in priv_dfs[1:]:\n",
    "        df_private = pd.concat([df_private, priv_df.reset_index(drop=True)], axis=0)\n",
    "   \n",
    "    return df_private.sort_values(levels[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "voPc2EQ5z0ff"
   },
   "source": [
    "This makes a differentially private copy of the dataframe as used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "private = make_private(df,geolevels[1:-1], geom_noise, [.5]*5,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "pulaski_dp.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
